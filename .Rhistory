#Alternative hypothesis: the data are not normally distributed
shapiro.test(zzzz)
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/Construction.xlsx")
#library(dplyr)
#zzzz <- na.omit(AllOther_Mod_Manu, cols=5)
#zzzz<- zzzz%>% mutate_at(c('Value'), as.numeric)
summary(Manu[2])
a <- Manu[2]
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
summary(Manu[2]) [3]
azaa <- 98.15
#Null hypothesis: the data are normally distributed
#Alternative hypothesis: the data are not normally distributed
azaa <- 111.84
cha<-Manu[2]
che<-function(cha){
ifelse(Manu[2] > azaa ,print(1),print(0))
}
df<-che(cha)
Manu[2]<-df
data5 <- Manu
data5  <- data5 [,c(-1)]
data5 <- as.data.frame(data5)
class(data5)
colnames(data5)[1] <- 'Totalfactorproductivity'
fit<-glm(formula = Totalfactorproductivity~ .,data = data5 ,family = binomial)
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/Construction.xlsx")
#library(dplyr)
#zzzz <- na.omit(AllOther_Mod_Manu, cols=5)
#zzzz<- zzzz%>% mutate_at(c('Value'), as.numeric)
summary(Manu[2])
a <- Manu[2]
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
summary(Manu[2]) [3]
azaa <- 111.84
#Null hypothesis: the data are normally distributed
#Alternative hypothesis: the data are not normally distributed
cha<-Manu[2]
che<-function(cha){
ifelse(Manu[2] > azaa ,print(1),print(0))
}
df<-che(cha)
Manu[2]<-df
View(Manu)
data5 <- Manu
data5  <- data5 [,c(-1)]
data5 <- as.data.frame(data5)
class(data5)
colnames(data5)[1] <- 'Totalfactorproductivity'
fit<-glm(formula = Totalfactorproductivity~ .,data = data5 ,family = binomial)
View(data5)
first_column <- coef(fit)
dummy_first_column <- coef(fit)
data6 <- cbind(first_column,dummy_first_column)
data6 <- as.data.frame(data6)
is.data.frame(data6)
#data6 <- as.data.frame(cbind((first_column,dummy_first_column)))
#data6 <- na.omit(data.frame(cbind(first_column, dummy_first_column)))
cha<-data6$dummy_first_column
che<-function(cha){
ifelse(cha > 0,print("Positive"),print("Negative"))
}
df<-che(cha)
data6$dummy_first_column<-df
third_column <- as.data.frame((summary(fit)[12]))
data66 <- cbind(data6, third_column)
first_column <- coef(fit)
dummy_first_column <- coef(fit)
data6 <- cbind(first_column,dummy_first_column)
data6 <- as.data.frame(data6)
is.data.frame(data6)
#data6 <- as.data.frame(cbind((first_column,dummy_first_column)))
#data6 <- na.omit(data.frame(cbind(first_column, dummy_first_column)))
cha<-data6$dummy_first_column
che<-function(cha){
ifelse(cha > 0,print("Positive"),print("Negative"))
}
df<-che(cha)
data6$dummy_first_column<-df
third_column <- as.data.frame((summary(fit)[12]))
#data66 <- cbind(data6, third_column)
data66
first_column <- coef(fit)
dummy_first_column <- coef(fit)
data6 <- cbind(first_column,dummy_first_column)
data6 <- as.data.frame(data6)
is.data.frame(data6)
#data6 <- as.data.frame(cbind((first_column,dummy_first_column)))
#data6 <- na.omit(data.frame(cbind(first_column, dummy_first_column)))
cha<-data6$dummy_first_column
che<-function(cha){
ifelse(cha > 0,print("Positive"),print("Negative"))
}
df<-che(cha)
data6$dummy_first_column<-df
third_column <- as.data.frame((summary(fit)[12]))
#data66 <- cbind(data6, third_column)
#data66
#colnames(data66) <- c("coefficients", "PositiveorNegative","pvalues")
#is.data.frame(data66)
data7 <- as.data.frame(data66)
data7 <- as.data.frame(third_column)
index1 <- which(data7$pvalues > 0.05)
index2 <- which(data7$PositiveorNegative== "Negative")
data7[index1,]
data7[index2,]
dim(data7)            #original data to begin the question
dim(data7[index1,])  #number of rows with less than 0.05 p-value
dim(data7[index2,])  #variables that negatively contribute to salary.
#Reference: https://www.delftstack.com/howto/r/find-index-in-r/
View(third_column)
col(third_column) [4] <- "pvalues"
colanmes(third_column) [4] <- "pvalues"
colnames(third_column) [4] <- "pvalues"
colnames(third_column) [4] <- "pvalues"
data7 <- as.data.frame(third_column)
index1 <- which(data7$pvalues > 0.05)
index2 <- which(data7$PositiveorNegative== "Negative")
data7[index1,]
data7[index2,]
dim(data7)            #original data to begin the question
dim(data7[index1,])  #number of rows with less than 0.05 p-value
dim(data7[index2,])  #variables that negatively contribute to salary.
#Reference: https://www.delftstack.com/howto/r/find-index-in-r/
View(data6)
View(data6)
colnames(third_column) [4] <- "pvalues"
data7 <- as.data.frame(third_column)
index1 <- which(data7$pvalues > 0.05)
index2 <- which(data6$dummy_first_column== "Negative")
data7[index1,]
data7[index2,]
dim(data7)            #original data to begin the question
dim(data7[index1,])  #number of rows with less than 0.05 p-value
dim(data7[index2,])  #variables that negatively contribute to salary.
#Reference: https://www.delftstack.com/howto/r/find-index-in-r/
```{r}
```{r}
library("writexl")
write_xlsx(data6,"C:\\Users\\Student.DESKTOP-H642391\\Box\\PhD @ AU\\Comprehensive Exam\\Section 2\\data66_cons.xlsx")
third_column <- as.data.frame(summary(fit)[12])
is.data.frame(third_column)
write_xlsx(third_column,"C:\\Users\\Student.DESKTOP-H642391\\Box\\PhD @ AU\\Comprehensive Exam\\Section 2\\data66_cons1.xlsx")
coef(fit)
summary(fit)[12]))
((summary(fit)[12]))
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/Manufacturing.xlsx")
#library(dplyr)
#zzzz <- na.omit(AllOther_Mod_Manu, cols=5)
#zzzz<- zzzz%>% mutate_at(c('Value'), as.numeric)
summary(Manu[2])
a <- Manu[2]
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
summary(Manu[2]) [3]
azaa <- 111.84
#Null hypothesis: the data are normally distributed
#Alternative hypothesis: the data are not normally distributed
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/Manufacturing.xlsx")
#library(dplyr)
#zzzz <- na.omit(AllOther_Mod_Manu, cols=5)
#zzzz<- zzzz%>% mutate_at(c('Value'), as.numeric)
summary(Manu[2])
a <- Manu[2]
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
summary(Manu[2]) [3]
azaa <- 111.84
#Null hypothesis: the data are normally distributed
#Alternative hypothesis: the data are not normally distributed
```{r}
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/Construction.xlsx")
summary(Manu[2])
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
summary(Manu[2])
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/RealEstate.xlsx")
#library(dplyr)
#zzzz <- na.omit(AllOther_Mod_Manu, cols=5)
#zzzz<- zzzz%>% mutate_at(c('Value'), as.numeric)
summary(Manu[2])
a <- Manu[2]
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
summary(Manu[2]) [3]
azaa <- 111.84
#Null hypothesis: the data are normally distributed
#Alternative hypothesis: the data are not normally distributed
cha<-Manu[2]
che<-function(cha){
ifelse(Manu[2] > azaa ,print(1),print(0))
}
df<-che(cha)
Manu[2]<-df
View(Manu)
azaa <- 91.12
cha<-Manu[2]
che<-function(cha){
ifelse(Manu[2] > azaa ,print(1),print(0))
}
df<-che(cha)
Manu[2]<-df
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/RealEstate.xlsx")
#library(dplyr)
#zzzz <- na.omit(AllOther_Mod_Manu, cols=5)
#zzzz<- zzzz%>% mutate_at(c('Value'), as.numeric)
summary(Manu[2])
a <- Manu[2]
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
summary(Manu[2]) [3]
azaa <- 91.12
#Null hypothesis: the data are normally distributed
#Alternative hypothesis: the data are not normally distributed
cha<-Manu[2]
che<-function(cha){
ifelse(Manu[2] > azaa ,print(1),print(0))
}
df<-che(cha)
Manu[2]<-df
data5 <- Manu
data5  <- data5 [,c(-1)]
data5 <- as.data.frame(data5)
class(data5)
colnames(data5)[1] <- 'Totalfactorproductivity'
fit<-glm(formula = Totalfactorproductivity~ .,data = data5 ,family = binomial)
coef(fit)
first_column <- coef(fit)
dummy_first_column <- coef(fit)
data6 <- cbind(first_column,dummy_first_column)
data6 <- as.data.frame(data6)
is.data.frame(data6)
#data6 <- as.data.frame(cbind((first_column,dummy_first_column)))
#data6 <- na.omit(data.frame(cbind(first_column, dummy_first_column)))
cha<-data6$dummy_first_column
che<-function(cha){
ifelse(cha > 0,print("Positive"),print("Negative"))
}
df<-che(cha)
data6$dummy_first_column<-df
third_column <- as.data.frame((summary(fit)[12]))
#data66 <- cbind(data6, third_column)
#data66
#colnames(data66) <- c("coefficients", "PositiveorNegative","pvalues")
#is.data.frame(data66)
first_column
View(data6)
colnames(third_column) [4] <- "pvalues"
data7 <- as.data.frame(third_column)
index1 <- which(data7$pvalues > 0.05)
index2 <- which(data6$dummy_first_column== "Negative")
data7[index1,]
data7[index2,]
dim(data7)            #original data to begin the question
dim(data7[index1,])  #number of rows with less than 0.05 p-value
dim(data7[index2,])  #variables that negatively contribute to salary.
#Reference: https://www.delftstack.com/howto/r/find-index-in-r/
library("writexl")
write_xlsx(data6,"C:\\Users\\Student.DESKTOP-H642391\\Box\\PhD @ AU\\Comprehensive Exam\\Section 2\\data66_realestate.xlsx")
third_column <- as.data.frame(summary(fit)[12])
is.data.frame(third_column)
write_xlsx(third_column,"C:\\Users\\Student.DESKTOP-H642391\\Box\\PhD @ AU\\Comprehensive Exam\\Section 2\\data66_realestat1.xlsx")
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/AllOther_Mod2.xlsx")
#library(dplyr)
#zzzz <- na.omit(AllOther_Mod_Manu, cols=5)
#zzzz<- zzzz%>% mutate_at(c('Value'), as.numeric)
summary(Manu[2])
a <- Manu[2]
lapply(Manu, sd)
lapply(Manu[2], shapiro.test)
View(Manu)
summary(Manu[5])
a <- Manu[5]
lapply(Manu[5], sd)
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/AllOther_Mod2.xlsx")
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/AllOther_Mod2.xlsx")
library(readxl)
Manu <- read_excel("C:/Users/Student.DESKTOP-H642391/Box/PhD @ AU/Comprehensive Exam/Section 2/Final Data sets/AllOther_Mod2.xlsx")
summary(Manu[5])
lapply(Manu[5], sd)
set.seed(1)
library(readxl)
library(ggplot2)
library(caret)
library(gtExtras)
library(gt)
setwd("~/GitHub/Final_Project-Group_8")
dataformat<-  readline(prompt = "Rename the data file to datasets.xlsx or datasets.csv and then press 1 (for XLSX data) or 2 (for CSV): ")
if(dataformat=="1") {
userdata <- read_excel('datasets.xlsx')
} else if(dataformat=="2") {
userdata <- read.csv(file = 'datasets.csv')
} else {
error <- "Please check the file name and respective selection based on XLSX or CSV (XLSX=1 or CSV=2 only)"
error
}
##Step 2:Calculating the Intial Beta-Umer
yk <- as.matrix(userdata$yk)
xk <- as.matrix(userdata$xk)
x <- 1:20
xk <- matrix(c(xk, rep(1, length(x))), ncol = 2, nrow = 20)
summary(lm(yk~xk))
(beta <- MASS::ginv(t(xk) %*% xk) %*% t(xk) %*% yk)
##Step 3:Creating a Log function to calculate Logarithms of Negative Values-Umer
Log <- function(xk, base = exp(1)) {
LOG <- base::log(as.complex(xk), base = base)
if (all(Im(LOG) == 0)) { LOG <- Re(LOG) }
LOG }
##Step 4: Creating Optimizing Function- Made by Tonghui Li and then Modified by Umer to include capability of processing complex numbers
optimizing<-function(beta){
n = nrow(xk)
prob<-c()
b<-c()
n=length(yk)
for (i in 1:n){
prob[i]<-1/1+exp(t(-xk[i,])%*%beta)
}
for (i in 1:n){
b[i]<-(-yk[i]*log(prob[i])-((1-yk[i])*Log(xk=1-prob[i])))
}
resultant <- sum(b)
real <- (Re(resultant))^2
imaginary <- (Im(resultant))^2
sqrt(real+imaginary)
}
optimizing(beta)
betahat <- beta
userdata <- as.data.frame(userdata)
betan<-optim(par=beta,fn=optimizing)
result<-list(betahat,betan)
Bootstrap_CI <- function(alpha){
#Number of boostrap replications
B <- 20
#Compute the length of vector
n <- length(xk)/2
#Number of boostrap replications
B <- 20
#Confidence level
alpha <- alpha
#Initialisation
boot_beta <- rep(NA, B)
for (i in 1:B){
xk_star <- xk[sample(1:n, replace = TRUE)]
yk_star <- yk[sample(1:n, replace = TRUE)]
x <- 1:20
xk_star <- matrix(c(xk_star, rep(1, length(x))), ncol = 2, nrow = 20)
dim(xk_star)
suppressWarnings(boot_beta[i] <- MASS::ginv(t(xk_star) %*% xk_star) %*% t(xk_star) %*% yk_star)
}
quantile(boot_beta, c(alpha/2, 1 - alpha/2))
}
##Step 6: Plotting of the fitted logistic curve to the actual values-Umer
##Step 6.1:load dataset
a <- model.matrix(~xk+yk) [,-3]
data <- a
data <- as.data.frame(data)
a <- as.data.frame(a)
##Step 6.2: split dataset into training and testing set
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- as.data.frame(data[sample, ])
test <- as.data.frame(data[!sample, ])
##Step 6.3: fit logistic regression model
model <- glm(yk~xk1, family="binomial", data=train)
summary(model)
##Step 6.4:plot logistic regression curve
library(ggplot2)
ggplot(data, aes(x=xk1, y=yk)) +
geom_point(alpha=.5) +
stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))
##Step 7: Creating Confusion Matrix-Umer
##Step 7.1:use model to predict probability of default
predicted <- predict(model, test, type="response")
##Step 7.2:convert to 1's and 0's based on Cutoff Value
predicted1 <- ifelse(predicted > 0.5, 1, 0)
library(caret)
predicted1 <- as.factor(predicted1)
test$yk <- as.factor(test$yk)
confusionmatrix <- confusionMatrix(test$yk, predicted1)
zzz <- confusionmatrix[2]
TP <- zzz[[c(1, 1)]]
FP <- zzz[[c(1, 2)]]
FN <- zzz[[c(1, 3)]]
TN <- zzz[[c(1, 4)]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)
FNR <- FN/(FN+FP)
TNR <- TN/(TN+FP)
LRplus <- TPR/TNR
LRminus <- FNR/TNR
Prevalence <- confusionmatrix[[c(4, 8)]]
Accuracy <- confusionmatrix[[c(3, 1)]]
Sensitivity <- confusionmatrix[[c(4, 1)]]
Specificity <- confusionmatrix[[c(4, 2)]]
False_Discovery_Rate <- FP/(FN+TN)
Diagnostic_Odds_Ratio <- LRplus/LRminus
zz <- cbind(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio)
zz <- as.data.frame(zz)
library(gtExtras)
library(gt)
library(caret)
umer1 <- matrix(data=NA,nrow=6,ncol=9)
for (i in 1 : 9){
predicted1 <- ifelse(predicted > (i/10), 1, 0)
#create confusion matrix
predicted1 <- as.factor(predicted1)
test$yk <- as.factor(test$yk)
confusionmatrix <- confusionMatrix(test$yk, predicted1)
zzz <- confusionmatrix[2]
TP <- zzz[[c(1, 1)]]
FP <- zzz[[c(1, 2)]]
FN <- zzz[[c(1, 3)]]
TN <- zzz[[c(1, 4)]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)
FNR <- FN/(FN+FP)
TNR <- TN/(TN+FP)
LRplus <- TPR/TNR
LRminus <- FNR/TNR
Prevalence <- confusionmatrix[[c(4, 8)]]
Accuracy <- confusionmatrix[[c(3, 1)]]
Sensitivity <- confusionmatrix[[c(4, 1)]]
Specificity <- confusionmatrix[[c(4, 2)]]
False_Discovery_Rate <- FP/(FN+TN)
Diagnostic_Odds_Ratio <- LRplus/LRminus
is.finite(c(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio))
zz <- cbind(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio)
Metrics <- colnames(zz)
umer1[,i] <- zz
class(umer1)
}
umer2 <- umer1
umer2 <- as.data.frame(umer2)
colnames(umer2) <- c("0.1 Cutoff", "0.2 Cutoff","0.3 Cutoff","0.4 Cutoff","0.5 Cutoff","0.6 Cutoff","0.7 Cutoff","0.8 Cutoff","0.9 Cutoff")
umer0 <- c('Prevalence', 'Accuracy','Sensitivity','Specificity','False_Discovery_Rate','Diagnostic_Odds_Ratio')
umer2$Metrics <- umer0
umer3 <- cbind(umer0,umer2[,-10])
colnames(umer3) <- c("Metrics","0.1 Cutoff", "0.2 Cutoff","0.3 Cutoff","0.4 Cutoff","0.5 Cutoff","0.6 Cutoff","0.7 Cutoff","0.8 Cutoff","0.9 Cutoff")
library(gtExtras)
library(gt)
head(as.data.frame(umer3)) %>%
gt() %>%
gt_theme_nytimes() %>%
tab_header(title = "Metrics VS Prediction Cutoff")
##Step 8: Plot of any of the  metrics evaluated over a grid of cut-off values for prediction going from 0.1 to 0.9 with steps of 0.1-Umer
umer4 <- umer3
umer44 <- lapply(umer4,is.infinite)
metricscheck <- sum(umer44[[1]])
point1cutoff <- sum(umer44[[2]])
point2cutoff <- sum(umer44[[3]])
point3cutoff <- sum(umer44[[4]])
point4cutoff <- sum(umer44[[5]])
point5cutoff <- sum(umer44[[6]])
point6cutoff <- sum(umer44[[7]])
point7cutoff <- sum(umer44[[8]])
point8cutoff <- sum(umer44[[9]])
zzzz <- rbind(metricscheck, point1cutoff,point2cutoff,point3cutoff,point4cutoff,point5cutoff,point6cutoff,point7cutoff,point8cutoff,point9cutoff)
point9cutoff <- sum(umer44[[10]])
zzzz <- rbind(metricscheck, point1cutoff,point2cutoff,point3cutoff,point4cutoff,point5cutoff,point6cutoff,point7cutoff,point8cutoff,point9cutoff)
zzzz <- zzzz[-1,]
index1 <- which(zzzz!=1)
yaxis <- umer2[index1]
xaxis <- c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
xaxis <- xaxis[index1]
dim(xaxis)
dataforplot <- rbind(umer0,xaxis,yaxis)
xaxis
yaxis
zzzz <- zzzz[-1,]
zzzz <- zzzz[-1,]
index1 <- which(zzzz!=1)
index1
umer0
xaxis
yaxis
dataforplot <- cbind(umer0,xaxis,yaxis)
View(dataforplot)
point1cutoff
View(dataforplot)
colnames(dataforplot) <- c('Metrics', 'xaxis','yaxis')
View(userdata)
dataforplot1 <- as.data.frame(dataforplot)
ggplot(dataforplot1) +
geom_point(aes(y= yaxis,x=xaxis), color=Metrics)+ scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
ggplot(dataforplot1) +
geom_point(aes(y= yaxis,x=xaxis), color=dataforplot$Metrics)+ scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
rlang::last_error()
rlang::last_trace()
dataforplot1 <- as.data.frame(dataforplot)
ggplot(dataforplot1) +
geom_point(aes(y= yaxis,x=xaxis), color=dataforplot$Metrics)+ scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
ggplot(dataforplot1) +
geom_point(aes(y= yaxis,x=xaxis), color=dataforplot1$Metrics)+ scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
dataforplot1 <- as.data.frame(dataforplot)
ggplot(dataforplot1) +
geom_point(aes(y= yaxis,x=xaxis), color=Metrics)+ scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
colnames(dataforplot) <- c('Metrics', 'Cutoff_Value', 'Metrics_Value')
class(dataforplot)
ggplot(dataforplot) +
geom_point(aes(y= Metrics_Value, x=Cutoff_Value, color=Metrics))+scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
