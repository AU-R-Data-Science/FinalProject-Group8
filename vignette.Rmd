---
title: "Final Project"
author: "Muhammad Umer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r,eval=F, echo=T}
introDS::runShiny('rmd')
```
# Set Seed

The set seed is used to make the data analysis and as a matter of fact the research reproducible. In essence, allowing for reproducible research implies that anyone could run the code (knit the document, etc.) and obtain the same exact results as the original research and RMarkdown is commonly used to address this issue (Beckman et al.,2020). The user can input can a postive integer value in the parenthesis. 

```{r, eval=F, echo=T}
set.seed(1)
```

# Packages and Libraries to be Loaded
```{r, eval=F, echo=T}
install.packages("gtExtras")
pak::pak("caret")
install.packages("ggplot2")
library(readxl)
library(ggplot2)
library(caret)
library(gtExtras)
library(gt)
```

# Step 1: Asking the user to provide data in Excel or CSV format-Umer
```{r, eval=F, echo=T}
dataformat<-  readline(prompt = "Rename the data file to datasets.xlsx or datasets.csv and then press 1 (for XLSX data) or 2 (for CSV): ")

if(dataformat=="1") {
  userdata <- read_excel('datasets.xlsx')
} else if(dataformat=="2") {
  userdata <- read.csv(file = 'datasets.csv')
} else {
  error <- "Please check the file name and respective selection based on XLSX or CSV (XLSX=1 or CSV=2 only)"
  error
}
```

# Step 2:Calculating the Initial Beta-Umer
```{r,eval=F, echo=T}
yk <- as.matrix(userdata$yk)
xk <- as.matrix(userdata$xk)
x <- 1:20
xk <- matrix(c(xk, rep(1, length(x))), ncol = 2, nrow = 20)
summary(lm(yk~xk))
(beta <- MASS::ginv(t(xk) %*% xk) %*% t(xk) %*% yk)
```

# Step 3:Creating a Log function to calculate Logarithms of Negative Values-Umer
```{r,eval=F, echo=T}
Log <- function(xk, base = exp(1)) {
  LOG <- base::log(as.complex(xk), base = base)
  if (all(Im(LOG) == 0)) { LOG <- Re(LOG) }
  LOG }
```

# Step 4: Creating Optimizing Function- Made by Tonghui Li and then Modified by Umer to include capability of processing complex numbers
```{r,eval=F, echo=T}

optimizing<-function(beta){
  n = nrow(xk)
  prob<-c()
  b<-c()
  n=length(yk)
  for (i in 1:n){
    prob[i]<-1/1+exp(t(-xk[i,])%*%beta)
  }
  for (i in 1:n){
    b[i]<-(-yk[i]*log(prob[i])-((1-yk[i])*Log(xk=1-prob[i])))
  }
 resultant <- sum(b)
 real <- (Re(resultant))^2
 imaginary <- (Im(resultant))^2
 sqrt(real+imaginary)
}

optimizing(beta)
betahat <- beta
userdata <- as.data.frame(userdata)
betan<-optim(par=beta,fn=optimizing)
result<-list(betahat,betan)
```

# Step 5: Bootstrap Confidence Intervals-Umer
```{r,eval=F, echo=T}
Bootstrap_CI <- function(alpha){

#Number of boostrap replications
B <- 20

#Compute the length of vector
n <- length(xk)/2

#Number of boostrap replications
B <- 20

#Confidence level
alpha <- alpha

#Initialisation
boot_beta <- rep(NA, B)


for (i in 1:B){
  xk_star <- xk[sample(1:n, replace = TRUE)]
  yk_star <- yk[sample(1:n, replace = TRUE)]
  x <- 1:20
  xk_star <- matrix(c(xk_star, rep(1, length(x))), ncol = 2, nrow = 20)
  dim(xk_star)

  suppressWarnings(boot_beta[i] <- MASS::ginv(t(xk_star) %*% xk_star) %*% t(xk_star) %*% yk_star)
}

quantile(boot_beta, c(alpha/2, 1 - alpha/2))
}
```

# Step 6: Plotting of the fitted logistic curve to the actual values-Umer
```{r,eval=F, echo=T}
##Step 6.1:load dataset
a <- model.matrix(~xk+yk) [,-3]
data <- a
data <- as.data.frame(data)
a <- as.data.frame(a)


##Step 6.2: split dataset into training and testing set
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- as.data.frame(data[sample, ])
test <- as.data.frame(data[!sample, ])

##Step 6.3: fit logistic regression model
model <- glm(yk~xk1, family="binomial", data=train)
summary(model)

##Step 6.4:plot logistic regression curve
library(ggplot2)
ggplot(data, aes(x=xk1, y=yk)) +
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))
```

# Step 7: Creating Confusion Matrix-Umer
```{r,eval=F, echo=T }
##Step 7.1:use model to predict probability of default
predicted <- predict(model, test, type="response")

##Step 7.2:convert to 1's and 0's based on Cutoff Value
predicted1 <- ifelse(predicted > 0.5, 1, 0)

##Step 7.3:create confusion matrix
pak::pak("caret")
library(caret)
predicted1 <- as.factor(predicted1)
test$yk <- as.factor(test$yk)
confusionmatrix <- confusionMatrix(test$yk, predicted1)

zzz <- confusionmatrix[2]

TP <- zzz[[c(1, 1)]]
FP <- zzz[[c(1, 2)]]
FN <- zzz[[c(1, 3)]]
TN <- zzz[[c(1, 4)]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)

FNR <- FN/(FN+FP)
TNR <- TN/(TN+FP)

LRplus <- TPR/TNR
LRminus <- FNR/TNR

Prevalence <- confusionmatrix[[c(4, 8)]]
Accuracy <- confusionmatrix[[c(3, 1)]]
Sensitivity <- confusionmatrix[[c(4, 1)]]
Specificity <- confusionmatrix[[c(4, 2)]]
False_Discovery_Rate <- FP/(FN+TN)
Diagnostic_Odds_Ratio <- LRplus/LRminus

zz <- cbind(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio)
zz <- as.data.frame(zz)

install.packages("gtExtras")
library(gtExtras)
library(gt)
library(caret)
umer1 <- matrix(data=NA,nrow=6,ncol=9)

for (i in 1 : 9){

  predicted1 <- ifelse(predicted > (i/10), 1, 0)

  #create confusion matrix

  predicted1 <- as.factor(predicted1)
  test$yk <- as.factor(test$yk)
  confusionmatrix <- confusionMatrix(test$yk, predicted1)

  zzz <- confusionmatrix[2]

  TP <- zzz[[c(1, 1)]]
  FP <- zzz[[c(1, 2)]]
  FN <- zzz[[c(1, 3)]]
  TN <- zzz[[c(1, 4)]]
  TPR <- TP/(TP+FN)
  TNR <- TN/(TN+FP)

  FNR <- FN/(FN+FP)
  TNR <- TN/(TN+FP)

  LRplus <- TPR/TNR
  LRminus <- FNR/TNR

  Prevalence <- confusionmatrix[[c(4, 8)]]
  Accuracy <- confusionmatrix[[c(3, 1)]]
  Sensitivity <- confusionmatrix[[c(4, 1)]]
  Specificity <- confusionmatrix[[c(4, 2)]]
  False_Discovery_Rate <- FP/(FN+TN)
  Diagnostic_Odds_Ratio <- LRplus/LRminus

  is.finite(c(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio))

  zz <- cbind(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio)
  Metrics <- colnames(zz)
  umer1[,i] <- zz
class(umer1)
}

umer2 <- umer1
umer2 <- as.data.frame(umer2)
colnames(umer2) <- c("0.1 Cutoff", "0.2 Cutoff","0.3 Cutoff","0.4 Cutoff","0.5 Cutoff","0.6 Cutoff","0.7 Cutoff","0.8 Cutoff","0.9 Cutoff")
umer0 <- c('Prevalence', 'Accuracy','Sensitivity','Specificity','False_Discovery_Rate','Diagnostic_Odds_Ratio')
umer2$Metrics <- umer0
umer3 <- cbind(umer0,umer2[,-10])
colnames(umer3) <- c("Metrics","0.1 Cutoff", "0.2 Cutoff","0.3 Cutoff","0.4 Cutoff","0.5 Cutoff","0.6 Cutoff","0.7 Cutoff","0.8 Cutoff","0.9 Cutoff")

library(gtExtras)
library(gt)
head(as.data.frame(umer3)) %>%
gt() %>%
gt_theme_nytimes() %>%
tab_header(title = "Metrics VS Prediction Cutoff")
```

# Step 8: Plot of any of the  metrics evaluated over a grid of cut-off values for prediction going from 0.1 to 0.9 with steps of 0.1-Umer
```{r,eval=F, echo=T}

umer4 <- umer3
umer44 <- lapply(umer4,is.infinite)
metricscheck <- sum(umer44[[1]])
point1cutoff <- sum(umer44[[2]])
point2cutoff <- sum(umer44[[3]])
point3cutoff <- sum(umer44[[4]])
point4cutoff <- sum(umer44[[5]])
point5cutoff <- sum(umer44[[6]])
point6cutoff <- sum(umer44[[7]])
point7cutoff <- sum(umer44[[8]])
point8cutoff <- sum(umer44[[9]])
point9cutoff <- sum(umer44[[10]])

zzzz <- rbind(metricscheck, point1cutoff,point2cutoff,point3cutoff,point4cutoff,point5cutoff,point6cutoff,point7cutoff,point8cutoff,point9cutoff)
#zzzz <- zzzz[-1,]
index1 <- which(zzzz!=1)

yaxis <- umer2[index1]

xaxis <- c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
xaxis <- xaxis[index1]
dataforplot <- cbind(umer0,xaxis,yaxis) 
colnames(dataforplot) <- c('Metrics', 'Cutoff_Value', 'Metrics_Value')
class(dataforplot)
library(ggplot2)
ggplot(dataforplot) +
  geom_point(aes(y= Metrics_Value, x=Cutoff_Value, color=Metrics))+scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
```

