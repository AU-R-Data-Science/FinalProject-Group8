---
title: "Final Project_Group 8"
author: "Muhammad Umer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r,eval=F, echo=T}
introDS::runShiny('rmd')
```

# Set Seed

The set seed is used to make the data analysis and as a matter of fact the research reproducible. In essence, allowing for reproducible research implies that anyone could run the code (knit the document, etc.) and obtain the same exact results as the original research and RMarkdown is commonly used to address this issue (Beckman et al.,2020). The user can input can a positive integer value in the parenthesis. 

```{r, eval=F, echo=T}
set.seed(1)
```

# Packages and Libraries to be Loaded
The following is the list of packages to be installed and/or libraries to be loaded for the various processes to be executed.
```{r, eval=F, echo=T}
install.packages("gtExtras")
pak::pak("caret")
install.packages("ggplot2")
library(readxl)
library(ggplot2)
library(caret)
library(gtExtras)
library(gt)
```

# Step 1: Asking the user to provide data in Excel or CSV format-Umer
This is included to enable the user to provide the data set. The data set is provisioned for either XLSX or CSV based. The user is cautioned to rename the data set file in a particular naming convention and then select 1 for XLSX or 2 for CSV. If the user provides a file name other than specified or does not select 1 or 2, an error is generated. 
```{r, eval=F, echo=T}
dataformat<-  readline(prompt = "Rename the data file to datasets.xlsx or datasets.csv and then press 1 (for XLSX data) or 2 (for CSV): ")

if(dataformat=="1") {
  userdata <- read_excel('datasets.xlsx')
} else if(dataformat=="2") {
  userdata <- read.csv(file = 'datasets.csv')
} else {
  error <- "Please check the file name and respective selection based on XLSX or CSV (XLSX=1 or CSV=2 only)"
  error
}
```

# Step 2:Calculating the Initial Beta-Umer
The data from the user in Step 1 will be used to empirically calculate the beta for the linear model fitted on the data. The beta values are also verified via the summary function. 
```{r,eval=F, echo=T}
yk <- as.matrix(userdata$yk)
xk <- as.matrix(userdata$xk)
x <- 1:length(yk)
xx <- length(x)
xk <- matrix(c(xk, rep(1, length(x))), ncol = 2, nrow = xx)
summary(lm(yk~xk))
(beta <- MASS::ginv(t(xk) %*% xk) %*% t(xk) %*% yk)
```

# Step 3: Creating a Log function to calculate Logarithms of Negative Values-Umer
To able to compute the estimator to be computed using numerical optimization, it is necessary to allow the provision for a logarithmic value for negative number. This code allows the calculation logarithm of both positive and negative numbers.
```{r,eval=F, echo=T}
Log <- function(xk, base = exp(1)) {
  LOG <- base::log(as.complex(xk), base = base)
  if (all(Im(LOG) == 0)) { LOG <- Re(LOG) }
  LOG }
```

# Step 4: Creating Optimizing Function- Made by Tonghui Li and then Modified by Umer to include capability of processing complex numbers
The code computes the estimator to be computed using numerical optimization. It takes beta value from Step 2, x and y values provided by the user from Step 1.
```{r,eval=F, echo=T}

optimizing<-function(beta){
  n = nrow(xk)
  prob<-c()
  b<-c()
  n=length(yk)
  for (i in 1:n){
    prob[i]<-1/1+exp(t(-xk[i,])%*%beta)
  }
  for (i in 1:n){
    b[i]<-(-yk[i]*log(prob[i])-((1-yk[i])*Log(xk=1-prob[i])))
  }
 resultant <- sum(b)
 real <- (Re(resultant))^2
 imaginary <- (Im(resultant))^2
 sqrt(real+imaginary)
}

optimizing(beta)
betahat <- beta
userdata <- as.data.frame(userdata)
betan<-optim(par=beta,fn=optimizing)
result<-list(betahat,betan)
```

# Step 5: Bootstrap Confidence Intervals-Umer
This code allows the user to choose (i) the significance level α to obtain for the 1−α confidence intervals for β, and (ii) the number of bootstraps which by default will be 20.
```{r,eval=F, echo=T}
Bootstrap_CI <- function(alpha){

#Number of boostrap replications
B <- 20

#Compute the length of vector
n <- length(xk)/2

#Number of boostrap replications
B <- 20

#Confidence level
alpha <- alpha

#Initialisation
boot_beta <- rep(NA, B)


for (i in 1:B){
  xk_star <- xk[sample(1:n, replace = TRUE)]
  yk_star <- yk[sample(1:n, replace = TRUE)]
  x <- 1:20
  xk_star <- matrix(c(xk_star, rep(1, length(x))), ncol = 2, nrow = 20)
  dim(xk_star)

  suppressWarnings(boot_beta[i] <- MASS::ginv(t(xk_star) %*% xk_star) %*% t(xk_star) %*% yk_star)
}

quantile(boot_beta, c(alpha/2, 1 - alpha/2))
}
```

# Step 6: Plotting of the fitted logistic curve to the actual values-Umer
The code plots the logistic curve. The code separates the data into train and test data (70:30) probability. 
```{r,eval=F, echo=T}
##Step 6.1:load dataset
a <- model.matrix(~xk+yk) [,-3]
data <- a
data <- as.data.frame(data)
a <- as.data.frame(a)

##Step 6.2: split dataset into training and testing set
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- as.data.frame(data[sample, ])
test <- as.data.frame(data[!sample, ])

##Step 6.3: fit logistic regression model
model <- glm(yk~xk1, family="binomial", data=train)
summary(model)

##Step 6.4:plot logistic regression curve
library(ggplot2)
ggplot(data, aes(x=xk1, y=yk)) +
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))
```

# Step 7: Creating Confusion Matrix-Umer
This code formulates a Confusion Matrix for the data along side it outputs various metrics of a respective confusion matrix of cut-off values for prediction going from 0.1 to 0.9 with steps of 0.1.
```{r,eval=F, echo=T }
##Step 7.1:use model to predict probability of default
predicted <- predict(model, test, type="response")

##Step 7.2:convert to 1's and 0's based on Cutoff Value
predicted1 <- ifelse(predicted > 0.5, 1, 0)

##Step 7.3:create confusion matrix
pak::pak("caret")
library(caret)
predicted1 <- as.factor(predicted1)
test$yk <- as.factor(test$yk)
confusionmatrix <- confusionMatrix(test$yk, predicted1)

zzz <- confusionmatrix[2]

TP <- zzz[[c(1, 1)]]
FP <- zzz[[c(1, 2)]]
FN <- zzz[[c(1, 3)]]
TN <- zzz[[c(1, 4)]]
TPR <- TP/(TP+FN)
TNR <- TN/(TN+FP)

FNR <- FN/(FN+FP)
TNR <- TN/(TN+FP)

LRplus <- TPR/TNR
LRminus <- FNR/TNR

Prevalence <- confusionmatrix[[c(4, 8)]]
Accuracy <- confusionmatrix[[c(3, 1)]]
Sensitivity <- confusionmatrix[[c(4, 1)]]
Specificity <- confusionmatrix[[c(4, 2)]]
False_Discovery_Rate <- FP/(FN+TN)
Diagnostic_Odds_Ratio <- LRplus/LRminus

zz <- cbind(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio)
zz <- as.data.frame(zz)

install.packages("gtExtras")
library(gtExtras)
library(gt)
library(caret)
umer1 <- matrix(data=NA,nrow=6,ncol=9)

for (i in 1 : 9){

  predicted1 <- ifelse(predicted > (i/10), 1, 0)

  #create confusion matrix

  predicted1 <- as.factor(predicted1)
  test$yk <- as.factor(test$yk)
  confusionmatrix <- confusionMatrix(test$yk, predicted1)

  zzz <- confusionmatrix[2]

  TP <- zzz[[c(1, 1)]]
  FP <- zzz[[c(1, 2)]]
  FN <- zzz[[c(1, 3)]]
  TN <- zzz[[c(1, 4)]]
  TPR <- TP/(TP+FN)
  TNR <- TN/(TN+FP)

  FNR <- FN/(FN+FP)
  TNR <- TN/(TN+FP)

  LRplus <- TPR/TNR
  LRminus <- FNR/TNR

  Prevalence <- confusionmatrix[[c(4, 8)]]
  Accuracy <- confusionmatrix[[c(3, 1)]]
  Sensitivity <- confusionmatrix[[c(4, 1)]]
  Specificity <- confusionmatrix[[c(4, 2)]]
  False_Discovery_Rate <- FP/(FN+TN)
  Diagnostic_Odds_Ratio <- LRplus/LRminus

  is.finite(c(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio))

  zz <- cbind(Prevalence,Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio)
  Metrics <- colnames(zz)
  umer1[,i] <- zz
class(umer1)
}

umer2 <- umer1
umer2 <- as.data.frame(umer2)
colnames(umer2) <- c("0.1 Cutoff", "0.2 Cutoff","0.3 Cutoff","0.4 Cutoff","0.5 Cutoff","0.6 Cutoff","0.7 Cutoff","0.8 Cutoff","0.9 Cutoff")
umer0 <- c('Prevalence', 'Accuracy','Sensitivity','Specificity','False_Discovery_Rate','Diagnostic_Odds_Ratio')
umer2$Metrics <- umer0
umer3 <- cbind(umer0,umer2[,-10])
colnames(umer3) <- c("Metrics","0.1 Cutoff", "0.2 Cutoff","0.3 Cutoff","0.4 Cutoff","0.5 Cutoff","0.6 Cutoff","0.7 Cutoff","0.8 Cutoff","0.9 Cutoff")

library(gtExtras)
library(gt)
head(as.data.frame(umer3)) %>%
gt() %>%
gt_theme_nytimes() %>%
tab_header(title = "Metrics VS Prediction Cutoff")
```

# Step 8: Plot of any of the  metrics evaluated over a grid of cut-off values for prediction going from 0.1 to 0.9 with steps of 0.1-Umer
The code uses the data from Step 7 and plots the metrics evaluated over a grid of cut-off values for prediction going from 0.1 to 0.9 with steps of 0.1. However, only those Metrics Values against Cutoff Prediction Value(s) are plotting for which all six metrics (i.e. Prevalence, Accuracy,Sensitivity,Specificity,False_Discovery_Rate,Diagnostic_Odds_Ratio) are computable.
```{r,eval=F, echo=T}
umer4 <- umer3
umer44 <- lapply(umer4,is.infinite)
metricscheck <- sum(umer44[[1]])
point1cutoff <- sum(umer44[[2]])
point2cutoff <- sum(umer44[[3]])
point3cutoff <- sum(umer44[[4]])
point4cutoff <- sum(umer44[[5]])
point5cutoff <- sum(umer44[[6]])
point6cutoff <- sum(umer44[[7]])
point7cutoff <- sum(umer44[[8]])
point8cutoff <- sum(umer44[[9]])
point9cutoff <- sum(umer44[[10]])

zzzz <- rbind(metricscheck, point1cutoff,point2cutoff,point3cutoff,point4cutoff,point5cutoff,point6cutoff,point7cutoff,point8cutoff,point9cutoff)
#zzzz <- zzzz[-1,]
index1 <- which(zzzz!=1)

yaxis <- umer2[index1]

xaxis <- c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
xaxis <- xaxis[index1]
dataforplot <- cbind(umer0,xaxis,yaxis) 
colnames(dataforplot) <- c('Metrics', 'Cutoff_Value', 'Metrics_Value')
class(dataforplot)
library(ggplot2)
ggplot(dataforplot) +
  geom_point(aes(y= Metrics_Value, x=Cutoff_Value, color=Metrics))+scale_y_continuous(breaks=seq(0,1,0.1)) + scale_x_continuous(name='Cutoff Value for Prediction', limits=c(0.1,0.9), breaks=seq(0.1,0.9,0.1))+ggtitle("Metrics Values VS Cutoff Prediction Value(s)-for which All Six Metrics are Computable")
```

